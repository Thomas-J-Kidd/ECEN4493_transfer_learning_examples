{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjVtQfBVb6IvkMUxq4BatH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thomas-J-Kidd/ECEN4493_transfer_learning_examples/blob/main/transfer_learning_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import Necessary Libraries"
      ],
      "metadata": {
        "id": "cQbfl0ub07tk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xaIYGOkp02IW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define Data Preprocessing\n",
        "- Data preprocessing steps are defined to resize images for the model, convert them to tensors, and normalize their colors."
      ],
      "metadata": {
        "id": "bG6h6Fvt1BSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to fit MobileNetV2 input dimensions\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize images\n",
        "])\n"
      ],
      "metadata": {
        "id": "MPFcSFXT1DMP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load Dataset\n",
        "-     CIFAR-10 dataset is loaded with transformations applied.\n",
        "- Data loaders are created for both training and testing datasets with a specified batch size."
      ],
      "metadata": {
        "id": "yHMgleAm1GYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you're using CIFAR-10, but PyTorch's dataset class can be used for many other datasets.\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X1vBiEY1Pe9",
        "outputId": "df97d3a5-ac3f-468e-baae-a3a864bf5720"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 13089889.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Load Pre-trained Model and Modify It\n",
        "-     Load a pre-trained MobileNetV2 model.\n",
        "- Freeze its parameters to prevent them from being updated during training.\n",
        "- Modify the classifier layer to match the number of classes in the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "C2ke1L7k1Xzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers\n",
        "\n",
        "# Modify the last layer\n",
        "model.classifier[1] = nn.Linear(model.last_channel, 10)  # Assuming 10 classes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmNvHvu01dU5",
        "outputId": "0ee67fb9-13b1-4878-aa92-af8de4de540d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 127MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Define Loss Function and Optimizer\n",
        "-     CrossEntropyLoss is used for multi-class classification.\n",
        "- The Adam optimizer is set to optimize only the parameters of the last layer, which speeds up training and saves resources.\n"
      ],
      "metadata": {
        "id": "X6D1XMZw1fCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier[1].parameters(), lr=0.001)  # Optimize only the last layer\n"
      ],
      "metadata": {
        "id": "KoFfZjpn1juO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Train the Model\n",
        "-     Train the model for a defined number of epochs.\n",
        "- Iterate over the training dataset, compute the loss for each batch, and update the model's weights."
      ],
      "metadata": {
        "id": "oMm2zG071li5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-9Uafbw1oS_",
        "outputId": "0acb32aa-5d79-473b-e2db-ab8f9b5939d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7355582118034363\n",
            "Epoch 2, Loss: 0.6261181831359863\n",
            "Epoch 3, Loss: 0.6743506193161011\n",
            "Epoch 4, Loss: 0.5799391269683838\n",
            "Epoch 5, Loss: 0.7747159004211426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Evaluate the Model\n",
        "\n",
        "-     Evaluate the model's performance on the test dataset.\n",
        "- Compute the overall accuracy by comparing the model's predictions to the true labels."
      ],
      "metadata": {
        "id": "w_MHFazS1pkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9C-REgD1tk-",
        "outputId": "c2e10399-2fce-454f-d00f-ded1837a1bf9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 78.02%\n"
          ]
        }
      ]
    }
  ]
}